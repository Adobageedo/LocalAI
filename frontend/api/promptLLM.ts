import type { VercelRequest, VercelResponse } from '@vercel/node';
import llmClient, { ChatMessage, LLMRequest } from './utils/llmClient';

interface Attachment {
  filename: string;
  content: string;  // Base64 encoded
  mime_type?: string;
  size?: number;
}

interface StreamRequest {
  prompt?: string;
  messages?: ChatMessage[];
  systemPrompt?: string;
  maxTokens?: number;
  temperature?: number;
  rag?: boolean;           // <-- new flag
  ragCollection?: string;  // optional collection for RAG search
  topK?: number;           // optional top_k for RAG
  model?: string;
  useMcpTools?: boolean;   // <-- flag to enable/disable MCP tools (default: true)
  attachments?: Attachment[];  // <-- File attachments for backend processing
}

interface RagDoc {
  page_content: string;
  metadata: Record<string, any>;
}

const DEFAULT_USER_STYLE = `
Lâ€™analyse du style dâ€™Ã©criture de cet utilisateur rÃ©vÃ¨le un profil marquÃ© par une communication Ã  la fois professionnelle, concise mais aussi chaleureuse et attentive. Globalement, ses mails adoptent une structure claire, organisÃ©e en paragraphes courts et fluides, qui facilitent la lecture et la comprÃ©hension rapide des messages. La longueur moyenne de ses messages oscille souvent entre une et deux phrases principales, complÃ©tÃ©e par des formules de politesse simples mais efficaces, telles que Â« bonne journÃ©e Â», Â« merci Â» ou Â« bonne fin de journÃ©e Â», tÃ©moignant dâ€™un souci dâ€™entretien relationnel sans tomber dans lâ€™excÃ¨s de formules formelles. La signature, systÃ©matiquement prÃ©sente, reprend une formule standard avec ses coordonnÃ©es complÃ¨tes, ce qui renforce une image professionnelle, accessible et Ã  lâ€™Ã©coute.

Son vocabulaire sâ€™inscrit dans un registre principalement technique et administratif, utilisant des termes prÃ©cis et adaptÃ©s Ã  un contexte industriel ou de gestion de projets. Il privilÃ©gie la simplicitÃ© et la sobriÃ©tÃ©, Ã©vitant les tournures trop sophistiquÃ©es ou le jargon trop spÃ©cifique, mais sait aussi adapter ses expressions selon la situation, en Ã©tant parfois plus dÃ©taillÃ© lorsquâ€™il sâ€™agit dâ€™expliciter une dÃ©marche ou une demande prÃ©cise. La ponctuation est gÃ©nÃ©ralement sobre, mais il nâ€™hÃ©site pas Ã  employer des points pour sÃ©parer clairement les idÃ©es ou les Ã©tapes, ou des virgules pour fluidifier ses phrases. Lâ€™usage de formules de politesse en dÃ©but ou en fin dâ€™Ã©change est systÃ©matique, ce qui confÃ¨re Ã  sa communication un ton respectueux, poli mais naturel, Ã©vitant toute froideur.

Lâ€™utilisateur sait Ã©galement moduler son style en fonction des interlocuteurs ou du contexte : pour ses Ã©changes internes, avec des collÃ¨gues proches ou des partenaires rÃ©guliers, il privilÃ©gie la simplicitÃ©, la rapiditÃ© et une certaine familiaritÃ© dans ses formules, tout en maintenant un certain niveau de courtoisie. Par exemple, il peut commencer par un simple Â« Bonjour Â» ou Â« Salut Â», et conclure par Â« Bonne journÃ©e Â» ou Â« Bonne fin de journÃ©e Â», sans recours systÃ©matique Ã  des formules Ã©laborÃ©es. Lorsquâ€™il sâ€™agit de contacts plus hiÃ©rarchiquement Ã©loignÃ©s ou de partenaires externes, il adopte un ton plus formel, souvent en utilisant Â« Bonjour Â» ou Â« Bonjour Monsieur/Madame Â» en dÃ©but de mail, et en terminant par une formule de politesse plus soutenue, comme Â« Cordialement Â» ou Â« Bien Ã  vous Â». La tonalitÃ© reste respectueuse et professionnelle, mais il sait aussi faire preuve dâ€™une certaine chaleur relationnelle, notamment par des expressions telles que Â« merci Â» ou Â« bonne journÃ©e Â», qui tÃ©moignent de sa volontÃ© de maintenir une relation cordiale.

En matiÃ¨re de rÃ©activitÃ©, il privilÃ©gie la prÃ©cision et la clartÃ©. Dans ses rÃ©ponses rapides ou lors des relances, il va droit au but, en prÃ©cisant ses demandes ou en apportant les Ã©lÃ©ments indispensables, tout en restant poli. Lorsquâ€™il doit argumenter ou justifier une position, il nâ€™hÃ©site pas Ã  fournir des dÃ©tails ou Ã  expliquer la dÃ©marche, ce qui montre une attitude transparente et orientÃ©e vers la rÃ©solution. Il paraÃ®t Ã©galement attentif Ã  la relation, Ã©vitant toute forme de ton agressif ou de critique ouverte, prÃ©fÃ©rant insister sur la nÃ©cessitÃ© dâ€™Ã©changes constructifs ou de clarifications, tout en restant courtois.

Sa maniÃ¨re dâ€™adapter son style selon les situations est particuliÃ¨rement significative : en contexte interne ou avec des partenaires de confiance, il peut user dâ€™un ton plus direct, voire dÃ©contractÃ©, tout en conservant la politesse. En revanche, pour des Ã©changes formels ou avec de nouveaux contacts, il privilÃ©gie un ton plus protocolaire, avec des formules de politesse complÃ¨tes et une attention accrue Ã  la clartÃ©. La longueur de ses mails varie peu, mais il sait, quand la situation le demande, Ã©toffer ses messages pour apporter des justifications ou des prÃ©cisions, Ã©vitant ainsi toute ambiguÃ¯tÃ© ou incomprÃ©hension.

Il laisse Ã©galement transparaÃ®tre une volontÃ© dâ€™Ãªtre efficace, ne surchargeant pas ses messages dâ€™informations superflues, mais sans pour autant nÃ©gliger la prÃ©cision et la politesse. La tendance est Ã  la recherche dâ€™un Ã©quilibre subtil entre concision et courtoisie, avec un souci constant de maintenir de bonnes relations tout en Ã©tant clair et prÃ©cis dans ses demandes ou ses rÃ©ponses. En rÃ©sumÃ©, ce style tÃ©moigne dâ€™un professionnel rigoureux, respectueux, adaptable et soucieux de prÃ©server une relation cordiale avec ses interlocuteurs, tout en restant efficace et pragmatique dans sa communication Ã©crite.
`;


export default async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== 'POST') {
    res.status(405).json({ error: 'Method not allowed' });
    return;
  }

  try {
    const { 
      prompt, 
      messages, 
      systemPrompt, 
      maxTokens = 500, 
      temperature = 0.7,
      rag = false,
      ragCollection="edoardo",
      model = "gpt-4o-mini",
      useMcpTools = false,  // default to using MCP tools
      attachments  // File attachments
    } = req.body as StreamRequest;
    
    // Use a mutable variable for model selection downstream
    let modelToUse = model;

    if (!messages && (!prompt || !prompt.trim())) {
      res.status(400).json({ error: 'Either messages array or prompt is required' });
      return;
    }
    // Build conversation messages
    let conversationMessages: ChatMessage[] = messages ?? [
      ...(systemPrompt ? [{ role: 'system' as const, content: systemPrompt + DEFAULT_USER_STYLE }] : []),
      { role: 'user' as const, content: prompt! }
    ];

    // =====================
    // Attachment processing
    // =====================
    // Helpers for optional parsing of PDF and DOCX
    const extractPdfText = async (b64: string): Promise<string> => {
      try {
        // Dynamic import so function works even if dependency isn't present locally
        const pdfParse = await import('pdf-parse').catch(() => null as any);
        if (!pdfParse) {
          console.warn('â„¹ï¸ [Vercel promptLLM] pdf-parse not installed, skipping PDF extraction');
          return '';
        }
        const buffer = Buffer.from(b64, 'base64');
        const res: any = await (pdfParse as any).default(buffer);
        return (res && res.text) ? String(res.text) : '';
      } catch (err) {
        console.warn('âš ï¸ [Vercel promptLLM] PDF extraction failed:', err);
        return '';
      }
    };

    // Optional: Convert PDF (base64) to PNG base64 images using pdf2pic
    const convertPdfToPngBase64 = async (b64: string): Promise<string[]> => {
      try {
        const pdf2pic = await import('pdf2pic').catch(() => null as any);
        if (!pdf2pic) {
          console.warn('â„¹ï¸ [Vercel promptLLM] pdf2pic not installed, skipping PDF->image conversion');
          return [];
        }
        const { fromBuffer } = (pdf2pic as any);
        const buffer = Buffer.from(b64, 'base64');
        const options = {
          density: 200,
          format: 'png',
          width: 1600,
          height: 1600,
          savePath: undefined,
        };
        const convert = fromBuffer(buffer, options);
        // Convert only first page to minimize cost/time; extend to more if needed
        const result = await convert(1, { responseType: 'base64' }).catch(() => null as any);
        if (result?.base64) {
          const base64 = String(result.base64).replace(/\s/g, '');
          console.log('âœ… [Vercel promptLLM] PDF->image conversion succeeded (page 1)');
          return [base64];
        }
        console.warn('âš ï¸ [Vercel promptLLM] PDF->image conversion returned no base64');
        return [];
      } catch (err) {
        console.warn('âš ï¸ [Vercel promptLLM] PDF->image conversion failed:', (err as any)?.message || err);
        return [];
      }
    };

    const extractDocxText = async (b64: string): Promise<string> => {
      try {
        const mammoth = await import('mammoth').catch(() => null as any);
        if (!mammoth) {
          console.warn('â„¹ï¸ [Vercel promptLLM] mammoth not installed, skipping DOCX extraction');
          return '';
        }
        const buffer = Buffer.from(b64, 'base64');
        const result: any = await (mammoth as any).extractRawText({ buffer });
        return (result && result.value) ? String(result.value) : '';
      } catch (err) {
        console.warn('âš ï¸ [Vercel promptLLM] DOCX extraction failed:', err);
        return '';
      }
    };

    const isTextBased = (filename?: string, mime?: string) => {
      const ext = (filename || '').toLowerCase();
      const m = (mime || '').toLowerCase();
      const textExts = ['.txt', '.md', '.csv', '.json', '.xml', '.log', '.rtf'];
      const textMimes = ['text/plain', 'text/markdown', 'text/csv', 'application/json', 'application/xml', 'text/xml'];
      return textExts.some(e => ext.endsWith(e)) || textMimes.includes(m);
    };

    const isImage = (filename?: string, mime?: string) => {
      const ext = (filename || '').toLowerCase();
      const m = (mime || '').toLowerCase();
      const imageExts = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'];
      return imageExts.some(e => ext.endsWith(e)) || m.startsWith('image/');
    };

    const base64ToUtf8 = (b64: string) => {
      try {
        return Buffer.from(b64, 'base64').toString('utf8');
      } catch (e) {
        console.warn('âš ï¸ [Vercel promptLLM] Failed to decode base64 to UTF-8');
        return '';
      }
    };

    if (attachments && attachments.length > 0) {
      console.log('ğŸ”„ [Vercel promptLLM] Processing attachments on Node side...');

      const textParts: string[] = [];
      const imageDataUrls: string[] = [];

      for (const att of attachments) {
        const fname = att.filename || 'unknown-file';
        const mime = att.mime_type || '';

        if (isTextBased(fname, mime)) {
          const decoded = base64ToUtf8(att.content || '');
          if (decoded) {
            textParts.push(`\n--- Content from ${fname} ---\n${decoded}`);
            console.log(`ğŸ“„ [Vercel promptLLM] Added text from ${fname} (${decoded.length} chars)`);
          } else {
            console.log(`âš ï¸ [Vercel promptLLM] Empty/undecodable text for ${fname}`);
          }
        } else if (fname.toLowerCase().endsWith('.pdf') || mime === 'application/pdf') {
          const pdfText = await extractPdfText(att.content || '');
          if (pdfText) {
            textParts.push(`\n--- Content from ${fname} (PDF) ---\n${pdfText}`);
            console.log(`ğŸ“„ [Vercel promptLLM] Extracted PDF text from ${fname} (${pdfText.length} chars)`);
          } else {
            console.log(`â„¹ï¸ [Vercel promptLLM] PDF had no extractable text: ${fname}. Trying PDF->image conversion...`);
            const images = await convertPdfToPngBase64(att.content || '');
            if (images.length > 0) {
              // Push as PNG data URLs for Vision
              for (const img of images) {
                imageDataUrls.push(`data:image/png;base64,${img}`);
              }
              console.log(`âœ… [Vercel promptLLM] Attached ${images.length} image(s) converted from PDF`);
            } else {
              console.log(`â„¹ï¸ [Vercel promptLLM] PDF->image conversion not available/failed for ${fname}`);
              textParts.push(`\n--- Attachment notice ---\nA PDF named "${fname}" was attached but could not be rendered as an image for Vision. Consider providing a renderable image or a text-based version.`);
            }
          }
        } else if (fname.toLowerCase().endsWith('.docx') || mime === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {
          const docxText = await extractDocxText(att.content || '');
          if (docxText) {
            textParts.push(`\n--- Content from ${fname} (DOCX) ---\n${docxText}`);
            console.log(`ğŸ“„ [Vercel promptLLM] Extracted DOCX text from ${fname} (${docxText.length} chars)`);
          } else {
            console.log(`â„¹ï¸ [Vercel promptLLM] DOCX had no extractable text: ${fname}`);
          }
        } else if (isImage(fname, mime)) {
          const dataUrl = `data:${mime || 'image/jpeg'};base64,${att.content}`;
          imageDataUrls.push(dataUrl);
          console.log(`ğŸ–¼ï¸ [Vercel promptLLM] Prepared image for GPT-Vision: ${fname}`);
        } else {
          console.log(`â„¹ï¸ [Vercel promptLLM] Unsupported file type for inline processing: ${fname} (${mime}). Skipping extraction.`);
        }
      }

      // Inject text parts into system message (prepend or create one)
      if (textParts.length > 0) {
        const contextText = `\n\n## Attached Documents:\n${textParts.join('\n')}`;
        const sysIdx = conversationMessages.findIndex(m => m.role === 'system');
        if (sysIdx >= 0) {
          const original = typeof conversationMessages[sysIdx].content === 'string' ? conversationMessages[sysIdx].content as string : '';
          conversationMessages[sysIdx] = {
            role: 'system',
            content: `${original}${contextText}`,
          } as ChatMessage;
        } else {
          conversationMessages.unshift({ role: 'system', content: (systemPrompt || '') + DEFAULT_USER_STYLE + contextText } as ChatMessage);
        }
        console.log(`âœ… [Vercel promptLLM] Injected ${textParts.length} text file(s) into system context`);
      }

      // Attach images to the last user message as multimodal for GPT-Vision
      if (imageDataUrls.length > 0) {
        // find last user message
        for (let i = conversationMessages.length - 1; i >= 0; i--) {
          if (conversationMessages[i].role === 'user') {
            const currentContent = conversationMessages[i].content;
            const multimodal: any[] = [
              { type: 'text', text: typeof currentContent === 'string' ? currentContent : JSON.stringify(currentContent) },
              ...imageDataUrls.map(url => ({ type: 'image_url', image_url: { url, detail: 'high' } }))
            ];
            (conversationMessages[i] as any).content = multimodal;
            console.log(`âœ… [Vercel promptLLM] Attached ${imageDataUrls.length} image(s) to last user message for GPT-Vision`);
            break;
          }
        }

        // Ensure a Vision-capable model (only if we actually attached images)
        if (!String(modelToUse).includes('gpt-4')) {
          console.log(`ğŸ”„ [Vercel promptLLM] Switching model to gpt-4o for Vision support`);
          modelToUse = 'gpt-4o';
        }
      }
    }

    // --- RAG Integration ---
    if (rag) {
      try {
        console.log(`ğŸ“¨ rag: Received ${prompt}`);
        let topK = 100;
        const ragResponse = await fetch(
          `${process.env.RAG_API_URL || 'https://easier-snappily-ansley.ngrok-free.dev/api/rag/search'}`,
          {
            method: 'POST',
            headers: { 
              'Content-Type': 'application/json',
              'x-api-key': process.env.RAG_API_KEY || 'W1eqZEROOsKw9gphfEYPvPYlHqS0lSAELjbYJCWqCxFl831wqSmwlXTht6t4ABO0'  // <-- add API key header
            },

            body: JSON.stringify({
              query: prompt || "default query",
              collection: "TEST_BAUX_Vincent", //ragCollection || "edoardo",
              top_k: topK,
              split_prompt: false,
              rerank: false,
              use_hyde: false
            })
          }
        );

        if (!ragResponse.ok) {
          console.warn('RAG API returned error', await ragResponse.text());
        } else {
          const ragData: any = await ragResponse.json();
          const docs: RagDoc[] = ragData.documents ?? [];

          // Prepend RAG content as a system message for context
          if (docs.length > 0) {
            const contextText = docs.map((d, i) => `Document ${i + 1}: ${d.page_content}`).join('\n\n');
            conversationMessages = [
              { role: 'system' as const, content: `Use the following RAG documents to answer the user query:\n\n${contextText}` },
              ...conversationMessages
            ];
          }
        }
      } catch (err) {
        console.error('RAG API call failed', err);
      }
    }

    // Set headers for SSE streaming
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    let fullText = '';
    let chunkNumber = 0;

    // Fetch MCP tools if enabled
    console.log("use mcp tool =", useMcpTools);
    let mcpTools: any[] | null = null;
    if (useMcpTools) {
      // dynamic import â€” works in CommonJS, ESM, Vercel, Railway
      const { getMcpTools } = await import("./utils/mcp");
      mcpTools = await getMcpTools();
    }

    // --- TWO-HOP FLOW: First non-streaming to detect tool calls ---
    if (mcpTools && mcpTools.length > 0) {
      console.log('ğŸ” First hop: checking for tool calls...');
      
      const firstResponse = await llmClient.generateWithTools({
        model: modelToUse,
        messages: conversationMessages,
        temperature,
        maxTokens,
        tools: mcpTools,
      });

      // If LLM decided to call tools, execute them
      if (firstResponse.tool_calls && firstResponse.tool_calls.length > 0) {
        console.log(`ğŸ”§ LLM wants to call ${firstResponse.tool_calls.length} tool(s)`);
        
        // Add the assistant's message with tool_calls to conversation
        conversationMessages.push({
          role: 'assistant',
          content: firstResponse.message?.content || '',
          ...(firstResponse.tool_calls && { tool_calls: firstResponse.tool_calls })
        } as any);

        // Execute each tool call and add results
        const { executeMcpTool } = await import('./utils/mcp');
        
        for (const toolCall of firstResponse.tool_calls) {
          const toolName = toolCall.function.name;
          const toolArgs = JSON.parse(toolCall.function.arguments);
          
          try {
            const toolResult = await executeMcpTool(toolName, toolArgs);
            
            // Add tool result to conversation
            conversationMessages.push({
              role: 'tool',
              content: JSON.stringify(toolResult),
              tool_call_id: toolCall.id,
              name: toolName
            });
          } catch (toolError) {
            console.error(`âŒ Tool execution failed:`, toolError);
            conversationMessages.push({
              role: 'tool',
              content: JSON.stringify({ error: toolError instanceof Error ? toolError.message : 'Unknown error' }),
              tool_call_id: toolCall.id,
              name: toolName
            });
          }
        }

        console.log('âœ… All tools executed, now streaming final response...');
      } else {
        console.log('ğŸ’¬ No tool calls, streaming direct response...');
      }
    }

    // --- SECOND HOP: Stream the final response ---
    for await (const chunk of llmClient.generateStream({
      model: modelToUse,
      messages: conversationMessages,
      temperature,
      maxTokens,
      tools: undefined,  // Don't offer tools again in final response
    })) {
      chunkNumber++;

      if (chunk.delta) {
        fullText += chunk.delta;

        res.write(`data: ${JSON.stringify({
          type: 'chunk',
          chunkNumber,
          delta: chunk.delta,
          done: false
        })}\n\n`);
      }

      if (chunk.done) {
        res.write(`data: ${JSON.stringify({
          type: 'done',
          chunkNumber,
          fullText
        })}\n\n`);
        res.end();
      }
    }

  } catch (error) {
    console.error('Streaming error:', error);
    res.write(`data: ${JSON.stringify({
      type: 'error',
      error: error instanceof Error ? error.message : 'Unknown error'
    })}\n\n`);
    res.end();
  }
}
