import type { VercelRequest, VercelResponse } from '@vercel/node';
import llmClient, { ChatMessage, LLMRequest } from './utils/llmClient';
import { getMcpTools } from "./utils/mcp";

interface StreamRequest {
  prompt?: string;
  messages?: ChatMessage[];
  systemPrompt?: string;
  maxTokens?: number;
  temperature?: number;
  rag?: boolean;           // <-- new flag
  ragCollection?: string;  // optional collection for RAG search
  topK?: number;           // optional top_k for RAG
  model?: string;
}

interface RagDoc {
  page_content: string;
  metadata: Record<string, any>;
}

const DEFAULT_USER_STYLE = `
Lâ€™analyse du style dâ€™Ã©criture de cet utilisateur rÃ©vÃ¨le un profil marquÃ© par une communication Ã  la fois professionnelle, concise mais aussi chaleureuse et attentive. Globalement, ses mails adoptent une structure claire, organisÃ©e en paragraphes courts et fluides, qui facilitent la lecture et la comprÃ©hension rapide des messages. La longueur moyenne de ses messages oscille souvent entre une et deux phrases principales, complÃ©tÃ©e par des formules de politesse simples mais efficaces, telles que Â« bonne journÃ©e Â», Â« merci Â» ou Â« bonne fin de journÃ©e Â», tÃ©moignant dâ€™un souci dâ€™entretien relationnel sans tomber dans lâ€™excÃ¨s de formules formelles. La signature, systÃ©matiquement prÃ©sente, reprend une formule standard avec ses coordonnÃ©es complÃ¨tes, ce qui renforce une image professionnelle, accessible et Ã  lâ€™Ã©coute.

Son vocabulaire sâ€™inscrit dans un registre principalement technique et administratif, utilisant des termes prÃ©cis et adaptÃ©s Ã  un contexte industriel ou de gestion de projets. Il privilÃ©gie la simplicitÃ© et la sobriÃ©tÃ©, Ã©vitant les tournures trop sophistiquÃ©es ou le jargon trop spÃ©cifique, mais sait aussi adapter ses expressions selon la situation, en Ã©tant parfois plus dÃ©taillÃ© lorsquâ€™il sâ€™agit dâ€™expliciter une dÃ©marche ou une demande prÃ©cise. La ponctuation est gÃ©nÃ©ralement sobre, mais il nâ€™hÃ©site pas Ã  employer des points pour sÃ©parer clairement les idÃ©es ou les Ã©tapes, ou des virgules pour fluidifier ses phrases. Lâ€™usage de formules de politesse en dÃ©but ou en fin dâ€™Ã©change est systÃ©matique, ce qui confÃ¨re Ã  sa communication un ton respectueux, poli mais naturel, Ã©vitant toute froideur.

Lâ€™utilisateur sait Ã©galement moduler son style en fonction des interlocuteurs ou du contexte : pour ses Ã©changes internes, avec des collÃ¨gues proches ou des partenaires rÃ©guliers, il privilÃ©gie la simplicitÃ©, la rapiditÃ© et une certaine familiaritÃ© dans ses formules, tout en maintenant un certain niveau de courtoisie. Par exemple, il peut commencer par un simple Â« Bonjour Â» ou Â« Salut Â», et conclure par Â« Bonne journÃ©e Â» ou Â« Bonne fin de journÃ©e Â», sans recours systÃ©matique Ã  des formules Ã©laborÃ©es. Lorsquâ€™il sâ€™agit de contacts plus hiÃ©rarchiquement Ã©loignÃ©s ou de partenaires externes, il adopte un ton plus formel, souvent en utilisant Â« Bonjour Â» ou Â« Bonjour Monsieur/Madame Â» en dÃ©but de mail, et en terminant par une formule de politesse plus soutenue, comme Â« Cordialement Â» ou Â« Bien Ã  vous Â». La tonalitÃ© reste respectueuse et professionnelle, mais il sait aussi faire preuve dâ€™une certaine chaleur relationnelle, notamment par des expressions telles que Â« merci Â» ou Â« bonne journÃ©e Â», qui tÃ©moignent de sa volontÃ© de maintenir une relation cordiale.

En matiÃ¨re de rÃ©activitÃ©, il privilÃ©gie la prÃ©cision et la clartÃ©. Dans ses rÃ©ponses rapides ou lors des relances, il va droit au but, en prÃ©cisant ses demandes ou en apportant les Ã©lÃ©ments indispensables, tout en restant poli. Lorsquâ€™il doit argumenter ou justifier une position, il nâ€™hÃ©site pas Ã  fournir des dÃ©tails ou Ã  expliquer la dÃ©marche, ce qui montre une attitude transparente et orientÃ©e vers la rÃ©solution. Il paraÃ®t Ã©galement attentif Ã  la relation, Ã©vitant toute forme de ton agressif ou de critique ouverte, prÃ©fÃ©rant insister sur la nÃ©cessitÃ© dâ€™Ã©changes constructifs ou de clarifications, tout en restant courtois.

Sa maniÃ¨re dâ€™adapter son style selon les situations est particuliÃ¨rement significative : en contexte interne ou avec des partenaires de confiance, il peut user dâ€™un ton plus direct, voire dÃ©contractÃ©, tout en conservant la politesse. En revanche, pour des Ã©changes formels ou avec de nouveaux contacts, il privilÃ©gie un ton plus protocolaire, avec des formules de politesse complÃ¨tes et une attention accrue Ã  la clartÃ©. La longueur de ses mails varie peu, mais il sait, quand la situation le demande, Ã©toffer ses messages pour apporter des justifications ou des prÃ©cisions, Ã©vitant ainsi toute ambiguÃ¯tÃ© ou incomprÃ©hension.

Il laisse Ã©galement transparaÃ®tre une volontÃ© dâ€™Ãªtre efficace, ne surchargeant pas ses messages dâ€™informations superflues, mais sans pour autant nÃ©gliger la prÃ©cision et la politesse. La tendance est Ã  la recherche dâ€™un Ã©quilibre subtil entre concision et courtoisie, avec un souci constant de maintenir de bonnes relations tout en Ã©tant clair et prÃ©cis dans ses demandes ou ses rÃ©ponses. En rÃ©sumÃ©, ce style tÃ©moigne dâ€™un professionnel rigoureux, respectueux, adaptable et soucieux de prÃ©server une relation cordiale avec ses interlocuteurs, tout en restant efficace et pragmatique dans sa communication Ã©crite.
`;


export default async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== 'POST') {
    res.status(405).json({ error: 'Method not allowed' });
    return;
  }

  try {
    const { 
      prompt, 
      messages, 
      systemPrompt, 
      maxTokens = 500, 
      temperature = 0.7,
      rag = false,
      ragCollection="edoardo",
      model = "gpt-5-nano"
    } = req.body as StreamRequest;

    if (!messages && (!prompt || !prompt.trim())) {
      res.status(400).json({ error: 'Either messages array or prompt is required' });
      return;
    }
    // Build conversation messages
    let conversationMessages: ChatMessage[] = messages ?? [
      ...(systemPrompt ? [{ role: 'system' as const, content: systemPrompt + DEFAULT_USER_STYLE }] : []),
      { role: 'user' as const, content: prompt! }
    ];

    // --- RAG Integration ---
    if (rag) {
      try {
        console.log(`ğŸ“¨ rag: Received ${prompt}`);
        let topK=10
        const ragResponse = await fetch(
          `${process.env.RAG_API_URL || 'https://easier-snappily-ansley.ngrok-free.dev/api/rag/search'}`,
          {
            method: 'POST',
            headers: { 
              'Content-Type': 'application/json',
              'x-api-key': process.env.RAG_API_KEY || 'W1eqZEROOsKw9gphfEYPvPYlHqS0lSAELjbYJCWqCxFl831wqSmwlXTht6t4ABO0'  // <-- add API key header
            },

            body: JSON.stringify({
              query: prompt || "default query",
              collection: ragCollection || "edoardo",
              top_k: topK,
              split_prompt: true,
              rerank: false,
              use_hyde: false
            })
          }
        );

        if (!ragResponse.ok) {
          console.warn('RAG API returned error', await ragResponse.text());
        } else {
          const ragData: any = await ragResponse.json();
          const docs: RagDoc[] = ragData.documents ?? [];

          // Prepend RAG content as a system message for context
          if (docs.length > 0) {
            const contextText = docs.map((d, i) => `Document ${i + 1}: ${d.page_content}`).join('\n\n');
            conversationMessages = [
              { role: 'system' as const, content: `Use the following RAG documents to answer the user query:\n\n${contextText}` },
              ...conversationMessages
            ];
          }
        }
      } catch (err) {
        console.error('RAG API call failed', err);
      }
    }

    // Set headers for SSE streaming
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    let fullText = '';
    let chunkNumber = 0;

    // Fetch MCP tools once
    const mcpTools = await getMcpTools();

    for await (const chunk of llmClient.generateStream({
      model,
      messages: conversationMessages,
      temperature,
      maxTokens,
      tools: mcpTools,
    })) {
      chunkNumber++;

      if (chunk.delta) {
        fullText += chunk.delta;

        res.write(`data: ${JSON.stringify({
          type: 'chunk',
          chunkNumber,
          delta: chunk.delta,
          done: false
        })}\n\n`);
      }

      if (chunk.done) {
        res.write(`data: ${JSON.stringify({
          type: 'done',
          chunkNumber,
          fullText
        })}\n\n`);
        res.end();
      }
    }

  } catch (error) {
    console.error('Streaming error:', error);
    res.write(`data: ${JSON.stringify({
      type: 'error',
      error: error instanceof Error ? error.message : 'Unknown error'
    })}\n\n`);
    res.end();
  }
}