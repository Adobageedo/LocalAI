import type { VercelRequest, VercelResponse } from '@vercel/node';
import llmClient, { ChatMessage, LLMRequest } from './utils/llmClient';

interface Attachment {
  filename: string;
  content: string;  // Base64 encoded
  mime_type?: string;
  size?: number;
}

interface StreamRequest {
  prompt?: string;
  messages?: ChatMessage[];
  systemPrompt?: string;
  maxTokens?: number;
  temperature?: number;
  rag?: boolean;           // <-- new flag
  ragCollection?: string;  // optional collection for RAG search
  topK?: number;           // optional top_k for RAG
  model?: string;
  useMcpTools?: boolean;   // <-- flag to enable/disable MCP tools (default: true)
  attachments?: Attachment[];  // <-- File attachments for backend processing
}

interface RagDoc {
  page_content: string;
  path: string;
}

const DEFAULT_USER_STYLE = `
L‚Äôanalyse du style d‚Äô√©criture de cet utilisateur r√©v√®le un profil marqu√© par une communication √† la fois professionnelle, concise mais aussi chaleureuse et attentive. Globalement, ses mails adoptent une structure claire, organis√©e en paragraphes courts et fluides, qui facilitent la lecture et la compr√©hension rapide des messages. La longueur moyenne de ses messages oscille souvent entre une et deux phrases principales, compl√©t√©e par des formules de politesse simples mais efficaces, telles que ¬´ bonne journ√©e ¬ª, ¬´ merci ¬ª ou ¬´ bonne fin de journ√©e ¬ª, t√©moignant d‚Äôun souci d‚Äôentretien relationnel sans tomber dans l‚Äôexc√®s de formules formelles. La signature, syst√©matiquement pr√©sente, reprend une formule standard avec ses coordonn√©es compl√®tes, ce qui renforce une image professionnelle, accessible et √† l‚Äô√©coute.

Son vocabulaire s‚Äôinscrit dans un registre principalement technique et administratif, utilisant des termes pr√©cis et adapt√©s √† un contexte industriel ou de gestion de projets. Il privil√©gie la simplicit√© et la sobri√©t√©, √©vitant les tournures trop sophistiqu√©es ou le jargon trop sp√©cifique, mais sait aussi adapter ses expressions selon la situation, en √©tant parfois plus d√©taill√© lorsqu‚Äôil s‚Äôagit d‚Äôexpliciter une d√©marche ou une demande pr√©cise. La ponctuation est g√©n√©ralement sobre, mais il n‚Äôh√©site pas √† employer des points pour s√©parer clairement les id√©es ou les √©tapes, ou des virgules pour fluidifier ses phrases. L‚Äôusage de formules de politesse en d√©but ou en fin d‚Äô√©change est syst√©matique, ce qui conf√®re √† sa communication un ton respectueux, poli mais naturel, √©vitant toute froideur.

L‚Äôutilisateur sait √©galement moduler son style en fonction des interlocuteurs ou du contexte : pour ses √©changes internes, avec des coll√®gues proches ou des partenaires r√©guliers, il privil√©gie la simplicit√©, la rapidit√© et une certaine familiarit√© dans ses formules, tout en maintenant un certain niveau de courtoisie. Par exemple, il peut commencer par un simple ¬´ Bonjour ¬ª ou ¬´ Salut ¬ª, et conclure par ¬´ Bonne journ√©e ¬ª ou ¬´ Bonne fin de journ√©e ¬ª, sans recours syst√©matique √† des formules √©labor√©es. Lorsqu‚Äôil s‚Äôagit de contacts plus hi√©rarchiquement √©loign√©s ou de partenaires externes, il adopte un ton plus formel, souvent en utilisant ¬´ Bonjour ¬ª ou ¬´ Bonjour Monsieur/Madame ¬ª en d√©but de mail, et en terminant par une formule de politesse plus soutenue, comme ¬´ Cordialement ¬ª ou ¬´ Bien √† vous ¬ª. La tonalit√© reste respectueuse et professionnelle, mais il sait aussi faire preuve d‚Äôune certaine chaleur relationnelle, notamment par des expressions telles que ¬´ merci ¬ª ou ¬´ bonne journ√©e ¬ª, qui t√©moignent de sa volont√© de maintenir une relation cordiale.

En mati√®re de r√©activit√©, il privil√©gie la pr√©cision et la clart√©. Dans ses r√©ponses rapides ou lors des relances, il va droit au but, en pr√©cisant ses demandes ou en apportant les √©l√©ments indispensables, tout en restant poli. Lorsqu‚Äôil doit argumenter ou justifier une position, il n‚Äôh√©site pas √† fournir des d√©tails ou √† expliquer la d√©marche, ce qui montre une attitude transparente et orient√©e vers la r√©solution. Il para√Æt √©galement attentif √† la relation, √©vitant toute forme de ton agressif ou de critique ouverte, pr√©f√©rant insister sur la n√©cessit√© d‚Äô√©changes constructifs ou de clarifications, tout en restant courtois.

Sa mani√®re d‚Äôadapter son style selon les situations est particuli√®rement significative : en contexte interne ou avec des partenaires de confiance, il peut user d‚Äôun ton plus direct, voire d√©contract√©, tout en conservant la politesse. En revanche, pour des √©changes formels ou avec de nouveaux contacts, il privil√©gie un ton plus protocolaire, avec des formules de politesse compl√®tes et une attention accrue √† la clart√©. La longueur de ses mails varie peu, mais il sait, quand la situation le demande, √©toffer ses messages pour apporter des justifications ou des pr√©cisions, √©vitant ainsi toute ambigu√Øt√© ou incompr√©hension.

Il laisse √©galement transpara√Ætre une volont√© d‚Äô√™tre efficace, ne surchargeant pas ses messages d‚Äôinformations superflues, mais sans pour autant n√©gliger la pr√©cision et la politesse. La tendance est √† la recherche d‚Äôun √©quilibre subtil entre concision et courtoisie, avec un souci constant de maintenir de bonnes relations tout en √©tant clair et pr√©cis dans ses demandes ou ses r√©ponses. En r√©sum√©, ce style t√©moigne d‚Äôun professionnel rigoureux, respectueux, adaptable et soucieux de pr√©server une relation cordiale avec ses interlocuteurs, tout en restant efficace et pragmatique dans sa communication √©crite.
`;


export default async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== 'POST') {
    res.status(405).json({ error: 'Method not allowed' });
    return;
  }

  try {
    const { 
      prompt, 
      messages, 
      systemPrompt, 
      maxTokens = 500, 
      temperature = 0.7,
      rag = false,
      ragCollection="edoardo",
      model = "gpt-4o-mini",
      useMcpTools = false,  // default to using MCP tools
      attachments  // File attachments
    } = req.body as StreamRequest;
    
    // Use a mutable variable for model selection downstream
    let modelToUse = model;

    if (!messages && (!prompt || !prompt.trim())) {
      res.status(400).json({ error: 'Either messages array or prompt is required' });
      return;
    }
    // Build conversation messages
    let conversationMessages: ChatMessage[] = messages ?? [
      ...(systemPrompt ? [{ role: 'system' as const, content: systemPrompt + DEFAULT_USER_STYLE }] : []),
      { role: 'user' as const, content: prompt! }
    ];

    // =====================
    // Attachment processing
    // =====================
    // Helpers for optional parsing of PDF and DOCX
    const extractPdfText = async (b64: string): Promise<string> => {
      try {
        // Dynamic import so function works even if dependency isn't present locally
        const pdfParse = await import('pdf-parse').catch(() => null as any);
        if (!pdfParse) {
          console.warn('‚ÑπÔ∏è [Vercel promptLLM] pdf-parse not installed, skipping PDF extraction');
          return '';
        }
        const buffer = Buffer.from(b64, 'base64');
        const res: any = await (pdfParse as any).default(buffer);
        return (res && res.text) ? String(res.text) : '';
      } catch (err) {
        console.warn('‚ö†Ô∏è [Vercel promptLLM] PDF extraction failed:', err);
        return '';
      }
    };

    // Optional: Convert PDF (base64) to PNG base64 images using pdf2pic
    const convertPdfToPngBase64 = async (b64: string): Promise<string[]> => {
      try {
        const pdf2pic = await import('pdf2pic').catch(() => null as any);
        if (!pdf2pic) {
          console.warn('‚ÑπÔ∏è [Vercel promptLLM] pdf2pic not installed, skipping PDF->image conversion');
          return [];
        }
        const { fromBuffer } = (pdf2pic as any);
        const buffer = Buffer.from(b64, 'base64');
        const options = {
          density: 200,
          format: 'png',
          width: 1600,
          height: 1600,
          savePath: undefined,
        };
        const convert = fromBuffer(buffer, options);
        // Convert only first page to minimize cost/time; extend to more if needed
        const result = await convert(1, { responseType: 'base64' }).catch(() => null as any);
        if (result?.base64) {
          const base64 = String(result.base64).replace(/\s/g, '');
          console.log('‚úÖ [Vercel promptLLM] PDF->image conversion succeeded (page 1)');
          return [base64];
        }
        console.warn('‚ö†Ô∏è [Vercel promptLLM] PDF->image conversion returned no base64');
        return [];
      } catch (err) {
        console.warn('‚ö†Ô∏è [Vercel promptLLM] PDF->image conversion failed:', (err as any)?.message || err);
        return [];
      }
    };

    const extractDocxText = async (b64: string): Promise<string> => {
      try {
        const mammoth = await import('mammoth').catch(() => null as any);
        if (!mammoth) {
          console.warn('‚ÑπÔ∏è [Vercel promptLLM] mammoth not installed, skipping DOCX extraction');
          return '';
        }
        const buffer = Buffer.from(b64, 'base64');
        const result: any = await (mammoth as any).extractRawText({ buffer });
        return (result && result.value) ? String(result.value) : '';
      } catch (err) {
        console.warn('‚ö†Ô∏è [Vercel promptLLM] DOCX extraction failed:', err);
        return '';
      }
    };

    const isTextBased = (filename?: string, mime?: string) => {
      const ext = (filename || '').toLowerCase();
      const m = (mime || '').toLowerCase();
      const textExts = ['.txt', '.md', '.csv', '.json', '.xml', '.log', '.rtf'];
      const textMimes = ['text/plain', 'text/markdown', 'text/csv', 'application/json', 'application/xml', 'text/xml'];
      return textExts.some(e => ext.endsWith(e)) || textMimes.includes(m);
    };

    const isImage = (filename?: string, mime?: string) => {
      const ext = (filename || '').toLowerCase();
      const m = (mime || '').toLowerCase();
      const imageExts = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'];
      return imageExts.some(e => ext.endsWith(e)) || m.startsWith('image/');
    };

    const base64ToUtf8 = (b64: string) => {
      try {
        return Buffer.from(b64, 'base64').toString('utf8');
      } catch (e) {
        console.warn('‚ö†Ô∏è [Vercel promptLLM] Failed to decode base64 to UTF-8');
        return '';
      }
    };

    if (attachments && attachments.length > 0) {
      console.log('üîÑ [Vercel promptLLM] Processing attachments on Node side...');

      const textParts: string[] = [];
      const imageDataUrls: string[] = [];

      for (const att of attachments) {
        const fname = att.filename || 'unknown-file';
        const mime = att.mime_type || '';

        if (isTextBased(fname, mime)) {
          const decoded = base64ToUtf8(att.content || '');
          if (decoded) {
            textParts.push(`\n--- Content from ${fname} ---\n${decoded}`);
            console.log(`üìÑ [Vercel promptLLM] Added text from ${fname} (${decoded.length} chars)`);
          } else {
            console.log(`‚ö†Ô∏è [Vercel promptLLM] Empty/undecodable text for ${fname}`);
          }
        } else if (fname.toLowerCase().endsWith('.pdf') || mime === 'application/pdf') {
          const pdfText = await extractPdfText(att.content || '');
          if (pdfText) {
            textParts.push(`\n--- Content from ${fname} (PDF) ---\n${pdfText}`);
            console.log(`üìÑ [Vercel promptLLM] Extracted PDF text from ${fname} (${pdfText.length} chars)`);
          } else {
            console.log(`‚ÑπÔ∏è [Vercel promptLLM] PDF had no extractable text: ${fname}. Trying PDF->image conversion...`);
            const images = await convertPdfToPngBase64(att.content || '');
            if (images.length > 0) {
              // Push as PNG data URLs for Vision
              for (const img of images) {
                imageDataUrls.push(`data:image/png;base64,${img}`);
              }
              console.log(`‚úÖ [Vercel promptLLM] Attached ${images.length} image(s) converted from PDF`);
            } else {
              console.log(`‚ÑπÔ∏è [Vercel promptLLM] PDF->image conversion not available/failed for ${fname}`);
              textParts.push(`\n--- Attachment notice ---\nA PDF named "${fname}" was attached but could not be rendered as an image for Vision. Consider providing a renderable image or a text-based version.`);
            }
          }
        } else if (fname.toLowerCase().endsWith('.docx') || mime === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {
          const docxText = await extractDocxText(att.content || '');
          if (docxText) {
            textParts.push(`\n--- Content from ${fname} (DOCX) ---\n${docxText}`);
            console.log(`üìÑ [Vercel promptLLM] Extracted DOCX text from ${fname} (${docxText.length} chars)`);
          } else {
            console.log(`‚ÑπÔ∏è [Vercel promptLLM] DOCX had no extractable text: ${fname}`);
          }
        } else if (isImage(fname, mime)) {
          const dataUrl = `data:${mime || 'image/jpeg'};base64,${att.content}`;
          imageDataUrls.push(dataUrl);
          console.log(`üñºÔ∏è [Vercel promptLLM] Prepared image for GPT-Vision: ${fname}`);
        } else {
          console.log(`‚ÑπÔ∏è [Vercel promptLLM] Unsupported file type for inline processing: ${fname} (${mime}). Skipping extraction.`);
        }
      }

      // Inject text parts into system message (prepend or create one)
      if (textParts.length > 0) {
        const contextText = `\n\n## Attached Documents:\n${textParts.join('\n')}`;
        const sysIdx = conversationMessages.findIndex(m => m.role === 'system');
        if (sysIdx >= 0) {
          const original = typeof conversationMessages[sysIdx].content === 'string' ? conversationMessages[sysIdx].content as string : '';
          conversationMessages[sysIdx] = {
            role: 'system',
            content: `${original}${contextText}`,
          } as ChatMessage;
        } else {
          conversationMessages.unshift({ role: 'system', content: (systemPrompt || '') + DEFAULT_USER_STYLE + contextText } as ChatMessage);
        }
        console.log(`‚úÖ [Vercel promptLLM] Injected ${textParts.length} text file(s) into system context`);
      }

      // Attach images to the last user message as multimodal for GPT-Vision
      if (imageDataUrls.length > 0) {
        // find last user message
        for (let i = conversationMessages.length - 1; i >= 0; i--) {
          if (conversationMessages[i].role === 'user') {
            const currentContent = conversationMessages[i].content;
            const multimodal: any[] = [
              { type: 'text', text: typeof currentContent === 'string' ? currentContent : JSON.stringify(currentContent) },
              ...imageDataUrls.map(url => ({ type: 'image_url', image_url: { url, detail: 'high' } }))
            ];
            (conversationMessages[i] as any).content = multimodal;
            console.log(`‚úÖ [Vercel promptLLM] Attached ${imageDataUrls.length} image(s) to last user message for GPT-Vision`);
            break;
          }
        }

        // Ensure a Vision-capable model (only if we actually attached images)
        if (!String(modelToUse).includes('gpt-4')) {
          console.log(`üîÑ [Vercel promptLLM] Switching model to gpt-4o for Vision support`);
          modelToUse = 'gpt-4o';
        }
      }
    }
    // --- RAG Integration ---
    if (rag) {
      try {
        console.log(`üì® rag: Received ${prompt}`);
        let topK = 100;
        const ragResponse = await fetch(
          `${process.env.RAG_API_URL || 'https://easier-snappily-ansley.ngrok-free.dev/api/rag/search'}`,
          {
            method: 'POST',
            headers: { 
              'Content-Type': 'application/json',
              'x-api-key': process.env.RAG_API_KEY || 'W1eqZEROOsKw9gphfEYPvPYlHqS0lSAELjbYJCWqCxFl831wqSmwlXTht6t4ABO0'  // <-- add API key header
            },

            body: JSON.stringify({
              query: prompt || "default query",
              collection: "TEST_BAUX_Vincent", //ragCollection || "edoardo",
              top_k: topK,
              split_prompt: false,
              rerank: false,
              use_hyde: false
            })
          }
        );

        if (!ragResponse.ok) {
          console.warn('RAG API returned error', await ragResponse.text());
        } else {
          const ragData: any = await ragResponse.json();
          const docs: RagDoc[] = ragData.documents ?? [];

          // Prepend RAG content as a system message for context
          if (docs.length > 0) {
            const contextText = docs.map((d, i) => `Document ${i + 1} "${d.path.split('/').pop()}" : ${d.page_content}`).join('\n\n');
            console.log(`RAG context: ${contextText}`);
            conversationMessages = [
              { role: 'system' as const, content: `Utilise prioritairement les documents RAG fournis, qui portent sur l‚Äôadministration de biens (baux, gestion locative, obligations des parties, etc.), pour r√©pondre √† la requ√™te de l‚Äôutilisateur.
Lorsque tu utilises une information provenant d‚Äôun document RAG, cite explicitement le nom du fichier ou de la source RAG en utilisant le nom du fichier ex "nom du fichier.pdf".

Si les documents RAG ne suffisent pas √† r√©pondre compl√®tement, tu peux compl√©ter avec des informations externes fiables.
Dans ce cas, tu dois obligatoirement citer clairement la source externe (ex. : code civil, service public, etc.).

N‚Äôinvente aucune information. Si une r√©ponse compl√®te n‚Äôest pas possible malgr√© l‚Äôusage du RAG et des sources externes, indique ce qui manque.:\n\n${contextText}` },
              ...conversationMessages
            ];
          }
        }
      } catch (err) {
        console.error('RAG API call failed', err);
      }
    }

    // Set headers for SSE streaming
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    let fullText = '';
    let chunkNumber = 0;

    // Fetch MCP tools if enabled
    console.log("use mcp tool =", useMcpTools);
    let mcpTools: any[] | null = null;
    if (useMcpTools) {
      // dynamic import ‚Äî works in CommonJS, ESM, Vercel, Railway
      const { getMcpTools } = await import("./utils/mcp");
      mcpTools = await getMcpTools();
    }

    // --- TWO-HOP FLOW: First non-streaming to detect tool calls ---
    if (mcpTools && mcpTools.length > 0) {
      console.log('üîç First hop: checking for tool calls...');
      
      const firstResponse = await llmClient.generateWithTools({
        model: modelToUse,
        messages: conversationMessages,
        temperature,
        maxTokens,
        tools: mcpTools,
      });

      // If LLM decided to call tools, execute them
      if (firstResponse.tool_calls && firstResponse.tool_calls.length > 0) {
        console.log(`üîß LLM wants to call ${firstResponse.tool_calls.length} tool(s)`);
        
        // Add the assistant's message with tool_calls to conversation
        conversationMessages.push({
          role: 'assistant',
          content: firstResponse.message?.content || '',
          ...(firstResponse.tool_calls && { tool_calls: firstResponse.tool_calls })
        } as any);

        // Execute each tool call and add results
        const { executeMcpTool } = await import('./utils/mcp');
        
        for (const toolCall of firstResponse.tool_calls) {
          const toolName = toolCall.function.name;
          const toolArgs = JSON.parse(toolCall.function.arguments);
          
          try {
            const toolResult = await executeMcpTool(toolName, toolArgs);
            
            // Add tool result to conversation
            conversationMessages.push({
              role: 'tool',
              content: JSON.stringify(toolResult),
              tool_call_id: toolCall.id,
              name: toolName
            });
          } catch (toolError) {
            console.error(`‚ùå Tool execution failed:`, toolError);
            conversationMessages.push({
              role: 'tool',
              content: JSON.stringify({ error: toolError instanceof Error ? toolError.message : 'Unknown error' }),
              tool_call_id: toolCall.id,
              name: toolName
            });
          }
        }

        console.log('‚úÖ All tools executed, now streaming final response...');
      } else {
        console.log('üí¨ No tool calls, streaming direct response...');
      }
    }

    // --- SECOND HOP: Stream the final response ---
    for await (const chunk of llmClient.generateStream({
      model: modelToUse,
      messages: conversationMessages,
      temperature,
      maxTokens,
      tools: undefined,  // Don't offer tools again in final response
    })) {
      chunkNumber++;

      if (chunk.delta) {
        fullText += chunk.delta;

        res.write(`data: ${JSON.stringify({
          type: 'chunk',
          chunkNumber,
          delta: chunk.delta,
          done: false
        })}\n\n`);
      }

      if (chunk.done) {
        res.write(`data: ${JSON.stringify({
          type: 'done',
          chunkNumber,
          fullText
        })}\n\n`);
        res.end();
      }
    }

  } catch (error) {
    console.error('Streaming error:', error);
    res.write(`data: ${JSON.stringify({
      type: 'error',
      error: error instanceof Error ? error.message : 'Unknown error'
    })}\n\n`);
    res.end();
  }
}
