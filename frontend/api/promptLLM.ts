import type { VercelRequest, VercelResponse } from '@vercel/node';
import llmClient, { ChatMessage, LLMRequest } from './utils/llmClient';
import { processAttachments } from './utils/attachmentPipeline';

interface Attachment {
  filename: string;
  content: string;  // Base64 encoded
  mime_type?: string;
  size?: number;
}

interface StreamRequest {
  prompt?: string;
  messages?: ChatMessage[];
  systemPrompt?: string;
  maxTokens?: number;
  temperature?: number;
  rag?: boolean;           // <-- new flag
  ragCollection?: string;  // optional collection for RAG search
  topK?: number;           // optional top_k for RAG
  model?: string;
  useMcpTools?: boolean;   // <-- flag to enable/disable MCP tools (default: true)
  attachments?: Attachment[];  // <-- File attachments for backend processing
}

interface RagDoc {
  page_content: string;
  metadata: Record<string, any>;
}

const DEFAULT_USER_STYLE = `
L‚Äôanalyse du style d‚Äô√©criture de cet utilisateur r√©v√®le un profil marqu√© par une communication √† la fois professionnelle, concise mais aussi chaleureuse et attentive. Globalement, ses mails adoptent une structure claire, organis√©e en paragraphes courts et fluides, qui facilitent la lecture et la compr√©hension rapide des messages. La longueur moyenne de ses messages oscille souvent entre une et deux phrases principales, compl√©t√©e par des formules de politesse simples mais efficaces, telles que ¬´ bonne journ√©e ¬ª, ¬´ merci ¬ª ou ¬´ bonne fin de journ√©e ¬ª, t√©moignant d‚Äôun souci d‚Äôentretien relationnel sans tomber dans l‚Äôexc√®s de formules formelles. La signature, syst√©matiquement pr√©sente, reprend une formule standard avec ses coordonn√©es compl√®tes, ce qui renforce une image professionnelle, accessible et √† l‚Äô√©coute.

Son vocabulaire s‚Äôinscrit dans un registre principalement technique et administratif, utilisant des termes pr√©cis et adapt√©s √† un contexte industriel ou de gestion de projets. Il privil√©gie la simplicit√© et la sobri√©t√©, √©vitant les tournures trop sophistiqu√©es ou le jargon trop sp√©cifique, mais sait aussi adapter ses expressions selon la situation, en √©tant parfois plus d√©taill√© lorsqu‚Äôil s‚Äôagit d‚Äôexpliciter une d√©marche ou une demande pr√©cise. La ponctuation est g√©n√©ralement sobre, mais il n‚Äôh√©site pas √† employer des points pour s√©parer clairement les id√©es ou les √©tapes, ou des virgules pour fluidifier ses phrases. L‚Äôusage de formules de politesse en d√©but ou en fin d‚Äô√©change est syst√©matique, ce qui conf√®re √† sa communication un ton respectueux, poli mais naturel, √©vitant toute froideur.

L‚Äôutilisateur sait √©galement moduler son style en fonction des interlocuteurs ou du contexte : pour ses √©changes internes, avec des coll√®gues proches ou des partenaires r√©guliers, il privil√©gie la simplicit√©, la rapidit√© et une certaine familiarit√© dans ses formules, tout en maintenant un certain niveau de courtoisie. Par exemple, il peut commencer par un simple ¬´ Bonjour ¬ª ou ¬´ Salut ¬ª, et conclure par ¬´ Bonne journ√©e ¬ª ou ¬´ Bonne fin de journ√©e ¬ª, sans recours syst√©matique √† des formules √©labor√©es. Lorsqu‚Äôil s‚Äôagit de contacts plus hi√©rarchiquement √©loign√©s ou de partenaires externes, il adopte un ton plus formel, souvent en utilisant ¬´ Bonjour ¬ª ou ¬´ Bonjour Monsieur/Madame ¬ª en d√©but de mail, et en terminant par une formule de politesse plus soutenue, comme ¬´ Cordialement ¬ª ou ¬´ Bien √† vous ¬ª. La tonalit√© reste respectueuse et professionnelle, mais il sait aussi faire preuve d‚Äôune certaine chaleur relationnelle, notamment par des expressions telles que ¬´ merci ¬ª ou ¬´ bonne journ√©e ¬ª, qui t√©moignent de sa volont√© de maintenir une relation cordiale.

En mati√®re de r√©activit√©, il privil√©gie la pr√©cision et la clart√©. Dans ses r√©ponses rapides ou lors des relances, il va droit au but, en pr√©cisant ses demandes ou en apportant les √©l√©ments indispensables, tout en restant poli. Lorsqu‚Äôil doit argumenter ou justifier une position, il n‚Äôh√©site pas √† fournir des d√©tails ou √† expliquer la d√©marche, ce qui montre une attitude transparente et orient√©e vers la r√©solution. Il para√Æt √©galement attentif √† la relation, √©vitant toute forme de ton agressif ou de critique ouverte, pr√©f√©rant insister sur la n√©cessit√© d‚Äô√©changes constructifs ou de clarifications, tout en restant courtois.

Sa mani√®re d‚Äôadapter son style selon les situations est particuli√®rement significative : en contexte interne ou avec des partenaires de confiance, il peut user d‚Äôun ton plus direct, voire d√©contract√©, tout en conservant la politesse. En revanche, pour des √©changes formels ou avec de nouveaux contacts, il privil√©gie un ton plus protocolaire, avec des formules de politesse compl√®tes et une attention accrue √† la clart√©. La longueur de ses mails varie peu, mais il sait, quand la situation le demande, √©toffer ses messages pour apporter des justifications ou des pr√©cisions, √©vitant ainsi toute ambigu√Øt√© ou incompr√©hension.

Il laisse √©galement transpara√Ætre une volont√© d‚Äô√™tre efficace, ne surchargeant pas ses messages d‚Äôinformations superflues, mais sans pour autant n√©gliger la pr√©cision et la politesse. La tendance est √† la recherche d‚Äôun √©quilibre subtil entre concision et courtoisie, avec un souci constant de maintenir de bonnes relations tout en √©tant clair et pr√©cis dans ses demandes ou ses r√©ponses. En r√©sum√©, ce style t√©moigne d‚Äôun professionnel rigoureux, respectueux, adaptable et soucieux de pr√©server une relation cordiale avec ses interlocuteurs, tout en restant efficace et pragmatique dans sa communication √©crite.
`;


export default async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== 'POST') {
    res.status(405).json({ error: 'Method not allowed' });
    return;
  }

  try {
    const { 
      prompt, 
      messages, 
      systemPrompt, 
      maxTokens = 500, 
      temperature = 0.7,
      rag = false,
      ragCollection="edoardo",
      model = "gpt-4o-mini",
      useMcpTools = false,  // default to using MCP tools
      attachments  // File attachments
    } = req.body as StreamRequest;
    
    // Use a mutable variable for model selection downstream
    let modelToUse = model;

    if (!messages && (!prompt || !prompt.trim())) {
      res.status(400).json({ error: 'Either messages array or prompt is required' });
      return;
    }
    // Build conversation messages
    let conversationMessages: ChatMessage[] = messages ?? [
      ...(systemPrompt ? [{ role: 'system' as const, content: systemPrompt + DEFAULT_USER_STYLE }] : []),
      { role: 'user' as const, content: prompt! }
    ];

    // =====================
    // Attachment processing
    // =====================

    if (attachments && attachments.length > 0) {
      const result = await processAttachments({
        attachments,
        conversationMessages,
        systemPrompt,
        defaultUserStyle: DEFAULT_USER_STYLE,
        model: modelToUse,
      });
      conversationMessages = result.conversationMessages;
      modelToUse = result.modelToUse;
    }

    // --- RAG Integration ---
    if (rag) {
      try {
        console.log(`üì® rag: Received ${prompt}`);
        let topK=10
        const ragResponse = await fetch(
          `${process.env.RAG_API_URL || 'https://easier-snappily-ansley.ngrok-free.dev/api/rag/search'}`,
          {
            method: 'POST',
            headers: { 
              'Content-Type': 'application/json',
              'x-api-key': process.env.RAG_API_KEY || 'W1eqZEROOsKw9gphfEYPvPYlHqS0lSAELjbYJCWqCxFl831wqSmwlXTht6t4ABO0'  // <-- add API key header
            },

            body: JSON.stringify({
              query: prompt || "default query",
              collection: ragCollection || "edoardo",
              top_k: topK,
              split_prompt: true,
              rerank: false,
              use_hyde: false
            })
          }
        );

        if (!ragResponse.ok) {
          console.warn('RAG API returned error', await ragResponse.text());
        } else {
          const ragData: any = await ragResponse.json();
          const docs: RagDoc[] = ragData.documents ?? [];

          // Prepend RAG content as a system message for context
          if (docs.length > 0) {
            const contextText = docs.map((d, i) => `Document ${i + 1}: ${d.page_content}`).join('\n\n');
            conversationMessages = [
              { role: 'system' as const, content: `Use the following RAG documents to answer the user query:\n\n${contextText}` },
              ...conversationMessages
            ];
          }
        }
      } catch (err) {
        console.error('RAG API call failed', err);
      }
    }

    // Set headers for SSE streaming
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    let fullText = '';
    let chunkNumber = 0;

    // Fetch MCP tools if enabled
    console.log("use mcp tool =", useMcpTools);
    let mcpTools: any[] | null = null;
    if (useMcpTools) {
      // dynamic import ‚Äî works in CommonJS, ESM, Vercel, Railway
      const { getMcpTools } = await import("./utils/mcp.js");
      mcpTools = await getMcpTools();
    }

    // --- TWO-HOP FLOW: First non-streaming to detect tool calls ---
    if (mcpTools && mcpTools.length > 0) {
      console.log('üîç First hop: checking for tool calls...');
      
      const firstResponse = await llmClient.generateWithTools({
        model: modelToUse,
        messages: conversationMessages,
        temperature,
        maxTokens,
        tools: mcpTools,
      });

      // If LLM decided to call tools, execute them
      if (firstResponse.tool_calls && firstResponse.tool_calls.length > 0) {
        console.log(`üîß LLM wants to call ${firstResponse.tool_calls.length} tool(s)`);
        
        // Add the assistant's message with tool_calls to conversation
        conversationMessages.push({
          role: 'assistant',
          content: firstResponse.message?.content || '',
          ...(firstResponse.tool_calls && { tool_calls: firstResponse.tool_calls })
        } as any);

        // Execute each tool call and add results
        const { executeMcpTool } = await import('./utils/mcp');
        
        for (const toolCall of firstResponse.tool_calls) {
          const toolName = toolCall.function.name;
          const toolArgs = JSON.parse(toolCall.function.arguments);
          
          try {
            const toolResult = await executeMcpTool(toolName, toolArgs);
            
            // Add tool result to conversation
            conversationMessages.push({
              role: 'tool',
              content: JSON.stringify(toolResult),
              tool_call_id: toolCall.id,
              name: toolName
            });
          } catch (toolError) {
            console.error(`‚ùå Tool execution failed:`, toolError);
            conversationMessages.push({
              role: 'tool',
              content: JSON.stringify({ error: toolError instanceof Error ? toolError.message : 'Unknown error' }),
              tool_call_id: toolCall.id,
              name: toolName
            });
          }
        }

        console.log('‚úÖ All tools executed, now streaming final response...');
      } else {
        console.log('üí¨ No tool calls, streaming direct response...');
      }
    }

    // --- SECOND HOP: Stream the final response ---
    for await (const chunk of llmClient.generateStream({
      model: modelToUse,
      messages: conversationMessages,
      temperature,
      maxTokens,
      tools: undefined,  // Don't offer tools again in final response
    })) {
      chunkNumber++;

      if (chunk.delta) {
        fullText += chunk.delta;

        res.write(`data: ${JSON.stringify({
          type: 'chunk',
          chunkNumber,
          delta: chunk.delta,
          done: false
        })}\n\n`);
      }

      if (chunk.done) {
        res.write(`data: ${JSON.stringify({
          type: 'done',
          chunkNumber,
          fullText
        })}\n\n`);
        res.end();
      }
    }

  } catch (error) {
    console.error('Streaming error:', error);
    res.write(`data: ${JSON.stringify({
      type: 'error',
      error: error instanceof Error ? error.message : 'Unknown error'
    })}\n\n`);
    res.end();
  }
}
